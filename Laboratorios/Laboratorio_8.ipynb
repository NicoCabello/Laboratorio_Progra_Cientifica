{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "97c30de09ab74ff8a8ead49b42589d52",
    "deepnote_cell_height": 156.390625,
    "deepnote_cell_type": "markdown",
    "id": "XUZ1dFPHzAHl"
   },
   "source": [
    "<h1><center>Laboratorio 8: Â¿SuperhÃ©roe o Villano? ðŸ¦¸</center></h1>\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de ProgramaciÃ³n CientÃ­fica para Ciencia de Datos</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "44f0e095cf894684b9513510e9517584",
    "deepnote_cell_type": "markdown",
    "id": "UD8X1uhGzAHq"
   },
   "source": [
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: MatÃ­as Rojas y Mauricio Araneda\n",
    "- Auxiliar: Ignacio Meza D.\n",
    "- Ayudante: Rodrigo Guerra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "630dba7238124007a872bee8e5ac1068",
    "deepnote_cell_height": 171.78125,
    "deepnote_cell_type": "markdown",
    "id": "tXflExjqzAHr"
   },
   "source": [
    "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serÃ¡n revisados\n",
    "\n",
    "- Nombre de alumno 1: NicolÃ¡s Cabello\n",
    "- Nombre de alumno 2: Esteban MuÃ±oz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b3307d6b91384799b5ffb79d87b50821",
    "deepnote_cell_height": 63,
    "deepnote_cell_type": "markdown",
    "id": "AD-V0bbZzAHr"
   },
   "source": [
    "### **Link de repositorio de GitHub:** `https://github.com/NicoCabello/Laboratorio_Progra_Cientifica`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "29929e237c7e4bd58e32e7014be15f5d",
    "deepnote_cell_type": "markdown",
    "id": "6uBLPj1PzAHs"
   },
   "source": [
    "## Temas a tratar\n",
    "\n",
    "- CodificaciÃ³n de texto usando Bag of Words.\n",
    "- BÃºsqueda del modelo Ã³ptimo de clasificaciÃ³n usando `GridSearch`\n",
    "- Uso de pipelines.\n",
    "\n",
    "## Reglas:\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- **Ausentes** deberÃ¡n realizar la actividad solos. \n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serÃ¡n respondidos por este medio.\n",
    "- Prohibidas las copias. \n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### Objetivos principales del laboratorio\n",
    "\n",
    "- Obtener caracteristicas a partir de texto usando `CountVectorizer`.\n",
    "- Fijar un pipeline con un modelo base que luego se irÃ¡ optimizando.\n",
    "- Comprender como realizar una bÃºsqueda de grilla sobre un conjunto de clasificadores e hiperparÃ¡metros usando `GridSearch`.\n",
    "\n",
    "El laboratorio deberÃ¡ ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al mÃ¡ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante mÃ¡s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f5ade8a9983a447fb8a70a3f15daee03",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "id": "MhISwri4zAHy"
   },
   "source": [
    "#Importamos librerias utiles ðŸ˜¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T00:08:16.884674Z",
     "start_time": "2021-03-29T00:08:16.349846Z"
    },
    "cell_id": "6b4998dbdd2b486f9a27fc4040ca9c26",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_height": 1509.1875,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     21.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 36497,
    "execution_start": 1637348694866,
    "id": "uyc33dKdzAHy",
    "outputId": "65ee0162-a9ce-4118-a842-b7c74e73200e",
    "source_hash": "7ce9748b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.11.0-py2.py3-none-any.whl (15.3 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.3 MB 5.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly) (8.1.0)\n",
      "Installing collected packages: plotly\n",
      "  Attempting uninstall: plotly\n",
      "    Found existing installation: plotly 5.5.0\n",
      "    Uninstalling plotly-5.5.0:\n",
      "      Successfully uninstalled plotly-5.5.0\n",
      "Successfully installed plotly-5.11.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting umap-learn\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88 kB 3.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.7.3)\n",
      "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.56.4)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1 MB 37.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (4.13.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
      "Building wheels for collected packages: umap-learn, pynndescent\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82830 sha256=9b36664f159c005c9fa1b5bbb670ef2bdf0d00693634cb0a621de5f0316015ae\n",
      "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55509 sha256=7ce53241304aee7b9a2661fec8d5b9537098b85b003453f84c5d98c8b9afdb8d\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/bc/eb/974072a56a7082a302f8b4be1ad6d21bf5019235c2eff65928\n",
      "Successfully built umap-learn pynndescent\n",
      "Installing collected packages: pynndescent, umap-learn\n",
      "Successfully installed pynndescent-0.5.8 umap-learn-0.5.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LibrerÃ­a Core del lab.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Pre-procesamiento\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ClasifaciÃ³n\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Metricas de evaluaciÃ³n\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# LibrerÃ­a para plotear\n",
    "!pip install --upgrade plotly\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Proyecciones en baja dimensionalidad: UMAP\n",
    "!pip install umap-learn\n",
    "\n",
    "# LibrerÃ­a para NLP\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  \n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "17fd1c4a7c1045a685dff360277240e7",
    "deepnote_cell_height": 82,
    "deepnote_cell_type": "markdown",
    "id": "xpOTbQcxbSiy"
   },
   "source": [
    "# 1. Â¿Quien es Bat Cow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "49b52b8931f04a2fa4287082eb3da154",
    "deepnote_cell_height": 347.8125,
    "deepnote_cell_type": "markdown",
    "id": "3Q93vbNS25bM"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.imgur.com/D9f1RHy.jpg\" width=\"350\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "98dbaf188dec4147b2b7e981fc64c61f",
    "deepnote_cell_height": 503.03125,
    "deepnote_cell_type": "markdown",
    "id": "jnmZfFpxTTYX"
   },
   "source": [
    "En vez de estar desarrollando las evaluaciones correspondientes a su curso, su profesor de catedra y su auxiliar discuten acerca la alineaciÃ³n (hÃ©roe o villano) del personaje de ficciÃ³n Bat-Cow. \n",
    "\n",
    "El cuerpo docente, no logra ponerse de acuerdo si el personaje es bueno, neutral o malo: el auxiliar plantea que Bat-cow posee una siniestra mirada, intrigante pero comÃºn caracterÃ­stica de los personajes malvados. \n",
    "Por otra parte, extendiendo las ideas de Rousseau, el profesor plantea que tal como los humanos no nacen malos, no existe motivo por el cual una vaca con superpoderes deba serlo.\n",
    "\n",
    "Sin embargo, ambos concuerdan que es difÃ­cil estimar la alineaciÃ³n solo usando los atributos fÃ­sicos, por lo que creen el anÃ¡lisis debe ser complementado aÃºn mÃ¡s antes de comunicarle los resultados a su estudiantado. Buscando mÃ¡s informaciÃ³n, ambos sujetos se percatan de la existencia de un excelente antecedente para estimar la alineaciÃ³n: la historia personal de cada superhÃ©roe o villano.\n",
    "\n",
    "Es por esto le solicitan que construya y optimice un clasificador basado en texto el cual analice la alineaciÃ³n de cada personaje basado en su historia personal.\n",
    "\n",
    "Para este laboratorio deben trabajar con los datos `df_comics.csv` y `comics_no_label.csv` subidos a u-cursos. El primero es un conjunto de datos que les servirÃ¡ para entrenar un modelo de clasificaciÃ³n, mientras que el segundo es un dataset con personajes de ficciÃ³n no etiquetados a predecir (sÃ­, aquÃ­ estÃ¡ la misteriosa Batcow).\n",
    "\n",
    "Para comenzar cargue los dataset seÃ±alados y visualice a travÃ©s de un head los atributos que poseen cada uno de los dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s7jnid2ZaRwI"
   },
   "outputs": [],
   "source": [
    "def mount_drive(path):\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(\"/content/drive\")\n",
    "        %cd {path}\n",
    "    except: \n",
    "        print('Ignorando conexiÃ³n drive-colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yT5-nEbyjYEe",
    "outputId": "89541c30-9505-4e71-8f34-32d1d5397568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/MDS7202/Laboratorio 8\n"
     ]
    }
   ],
   "source": [
    "mount_drive(\"/content/drive/My Drive/MDS7202/Laboratorio 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "c4b057f299d348fa839ac3c555241045",
    "deepnote_cell_height": 117,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 325,
    "execution_start": 1637348732856,
    "id": "bED3w3tDbSCf",
    "source_hash": "443d6e8"
   },
   "outputs": [],
   "source": [
    "df_comics = pd.read_csv('df_comics.csv')\n",
    "df_comics_no_label = pd.read_csv('comics_no_label.csv')\n",
    "df_comics = df_comics.dropna(subset=['history_text']) # eliminar ejemplos sin historia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "4c6367f4379042cc8515e0e86849acf3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "deepnote_cell_height": 323.1875,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     208.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 654,
    "execution_start": 1637348731943,
    "id": "fD7deGzGjO0L",
    "outputId": "75a56f4d-95bc-45a1-b630-cb6f8a57e5e5",
    "source_hash": "b986316d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ddc18216-357f-4b20-aa04-fabdedd0b104\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>real_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>overall_score</th>\n",
       "      <th>history_text</th>\n",
       "      <th>powers_text</th>\n",
       "      <th>intelligence_score</th>\n",
       "      <th>strength_score</th>\n",
       "      <th>speed_score</th>\n",
       "      <th>...</th>\n",
       "      <th>has_flight</th>\n",
       "      <th>has_accelerated_healing</th>\n",
       "      <th>has_weapons_master</th>\n",
       "      <th>has_intelligence</th>\n",
       "      <th>has_reflexes</th>\n",
       "      <th>has_super_speed</th>\n",
       "      <th>has_durability</th>\n",
       "      <th>has_stamina</th>\n",
       "      <th>has_agility</th>\n",
       "      <th>has_super_strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3-D Man</td>\n",
       "      <td>Delroy Garrett, Jr.</td>\n",
       "      <td>Delroy Garrett, Jr.</td>\n",
       "      <td>6</td>\n",
       "      <td>Delroy Garrett, Jr. grew up to become a track ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A-Bomb</td>\n",
       "      <td>Richard Milhouse Jones</td>\n",
       "      <td>Richard Milhouse Jones</td>\n",
       "      <td>20</td>\n",
       "      <td>Richard \"Rick\" Jones was orphaned at a young ...</td>\n",
       "      <td>On rare occasions, and through unusual circu...</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aa</td>\n",
       "      <td>Aa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>Aa is one of the more passive members of the P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Aaron Cash</td>\n",
       "      <td>Aaron Cash</td>\n",
       "      <td>Aaron Cash</td>\n",
       "      <td>5</td>\n",
       "      <td>Aaron Cash is the head of security at Arkham A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Aayla Secura</td>\n",
       "      <td>Aayla Secura</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>ayla Secura was a Rutian Twi'lek Jedi Knight (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 82 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ddc18216-357f-4b20-aa04-fabdedd0b104')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-ddc18216-357f-4b20-aa04-fabdedd0b104 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-ddc18216-357f-4b20-aa04-fabdedd0b104');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   Unnamed: 0          name               real_name               full_name  \\\n",
       "0           0       3-D Man     Delroy Garrett, Jr.     Delroy Garrett, Jr.   \n",
       "1           2        A-Bomb  Richard Milhouse Jones  Richard Milhouse Jones   \n",
       "2           3            Aa                      Aa                     NaN   \n",
       "3           4    Aaron Cash              Aaron Cash              Aaron Cash   \n",
       "4           5  Aayla Secura            Aayla Secura                     NaN   \n",
       "\n",
       "  overall_score                                       history_text  \\\n",
       "0             6  Delroy Garrett, Jr. grew up to become a track ...   \n",
       "1            20   Richard \"Rick\" Jones was orphaned at a young ...   \n",
       "2            12  Aa is one of the more passive members of the P...   \n",
       "3             5  Aaron Cash is the head of security at Arkham A...   \n",
       "4             8  ayla Secura was a Rutian Twi'lek Jedi Knight (...   \n",
       "\n",
       "                                         powers_text  intelligence_score  \\\n",
       "0                                                NaN                  85   \n",
       "1    On rare occasions, and through unusual circu...                  80   \n",
       "2                                                NaN                  80   \n",
       "3                                                NaN                  80   \n",
       "4                                                NaN                  90   \n",
       "\n",
       "   strength_score  speed_score  ...  has_flight  has_accelerated_healing  \\\n",
       "0              30           60  ...         0.0                      0.0   \n",
       "1             100           80  ...         0.0                      1.0   \n",
       "2              50           55  ...         0.0                      0.0   \n",
       "3              10           25  ...         0.0                      0.0   \n",
       "4              40           45  ...         0.0                      1.0   \n",
       "\n",
       "   has_weapons_master has_intelligence has_reflexes has_super_speed  \\\n",
       "0                 0.0              0.0          0.0             1.0   \n",
       "1                 0.0              0.0          1.0             1.0   \n",
       "2                 0.0              0.0          0.0             0.0   \n",
       "3                 1.0              0.0          0.0             0.0   \n",
       "4                 0.0              0.0          0.0             0.0   \n",
       "\n",
       "  has_durability has_stamina has_agility has_super_strength  \n",
       "0            0.0         0.0         0.0                1.0  \n",
       "1            1.0         1.0         1.0                1.0  \n",
       "2            0.0         0.0         0.0                0.0  \n",
       "3            0.0         0.0         0.0                0.0  \n",
       "4            0.0         0.0         1.0                0.0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# queda a labor de su equipo hacer el anÃ¡lisis exploratorio\n",
    "df_comics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2b5684743aec4d388edaa8f190652e9e",
    "deepnote_cell_height": 410,
    "deepnote_cell_type": "markdown",
    "id": "i4tFPrFA4_O5"
   },
   "source": [
    "## 1.1 ObtenciÃ³n de Features y Bag of Words\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media0.giphy.com/media/eIUpSyzwGp0YhAMTKr/200.gif\" width=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "39e5b3d761274681bdafab029d6ede31",
    "deepnote_cell_height": 561.859375,
    "deepnote_cell_type": "markdown",
    "id": "f_4NF0_V5XZ-"
   },
   "source": [
    "Primero que todo, deben obtener un vector de caracterÃ­sticas del atributo `history_text`, utilizando `Bag of Words`. En este atributo se presenta una breve descripciÃ³n de la historia de cada uno de los personajes de ficciÃ³n presentes en el dataset. \n",
    "\n",
    "Pero... antes de empezar, Â¿Que es `Bag of Words`?...\n",
    "\n",
    "`Bag of Words` es un modelo de conteo utilizado en Procesamiento de Lenguaje Natural (NLP) que tiene como objetivo generar una representaciÃ³n vectorial (vector de caracterÃ­sticas en nuestro cas) para cada documento a travÃ©s del conteo de las palabras que contienen. \n",
    "\n",
    "La siguiente figura muestra un ejemplo de `Bag of Words` en acciÃ³n:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://user.oc-static.com/upload/2020/10/23/16034397439042_surfin%20bird%20bow.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "Como pueden ver, el modelo de `Bag of Words` no resulta tan complicado, Â¿pero cÃ³mo lo aplicamos en python?. \n",
    "\n",
    "Como podrÃ¡n darse cuenta del ejemplo anterior, para facilitar el conteo serÃ¡ necesario transformar cada uno de los documentos en vectores, donde cada una de las posiciones posee un carÃ¡cter. Este proceso es conocido como **tokenizaciÃ³n** y lo podemos realizar de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "4225703f5ca94caeba76b5271167c8ec",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_height": 227.375,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     40.375
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1637346921830,
    "id": "eR7WeZnGjO0M",
    "outputId": "951b8fd0-eb3f-46a5-92df-0a32d4a30324",
    "source_hash": "57e4888a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'teacher', 'rocks', 'like', 'a', 'good', 'rock', '&', 'roll'],\n",
       " ['the', 'rock', 'is', 'the', 'best', 'actor', 'in', 'the', 'world']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['The teacher rocks like a good rock & roll',\n",
    "             'the rock is the best actor in the world']\n",
    "\n",
    "\n",
    "docs_tokenizados = [word_tokenize(doc) for doc in docs]\n",
    "docs_tokenizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38363e52900d4138aa13cfc88bb699dc",
    "deepnote_cell_height": 424.125,
    "deepnote_cell_type": "markdown",
    "id": "ZKUpO0XejO0M",
    "tags": []
   },
   "source": [
    "Podemos mejorar un poco mÃ¡s el proceso de tokenizaciÃ³n agregando \n",
    "\n",
    "- Stemming:  Definimos Stemming como un algoritmo basado en reglas que transforma las palabras a una forma general. Un ejemplo de stemming, es el siguiente:\n",
    "- EliminaciÃ³n de Stopwords: EliminaciÃ³n de palabras muy frecuentes que entorpecen la clasificaciÃ³n (por ejemplo, el, la los, la, etc...)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://devopedia.org/images/article/218/8583.1569386710.png\" width=\"300\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "281adedd8d3e4785931934eadd57abfb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_height": 690.9375,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     59.5625
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 36,
    "execution_start": 1637346924545,
    "id": "N7_JNvTMjO0M",
    "outputId": "276a8318-0662-4bc1-f412-ab71b5f3f912",
    "source_hash": "d7f59237",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['the', 'teacher', 'rock', 'like', 'good', 'rock', '&', 'roll'],\n",
       " ['rock', 'best', 'actor', 'world'],\n",
       " ['new', 'york', 'beauti', 'citi']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos algunas stopword que queremos que sean eliminadas\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Definimos un tokenizador con Stemming\n",
    "class StemmerTokenizer:\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        doc_tok = word_tokenize(doc)\n",
    "        doc_tok = [t for t in doc_tok if t not in stop_words]\n",
    "        return [self.ps.stem(t) for t in doc_tok]\n",
    "\n",
    "# Inicializamos tokenizador\n",
    "tokenizador = StemmerTokenizer()\n",
    "\n",
    "# Creamos algunos documentos\n",
    "docs = ['The teacher rocks like a good rock & roll',\n",
    "        'the rock is the best actor in the world',\n",
    "        'New York is a beautiful city']\n",
    "\n",
    "# Obtenemos el token del primer documento\n",
    "[tokenizador(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "3d0f1a2529964cedb72ccc62a8e818a5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_height": 192.5625,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     59.5625
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13,
    "execution_start": 1637346927213,
    "id": "TrXystxgjO0N",
    "outputId": "c3524606-de76-4462-eb44-e4f67dce9db9",
    "source_hash": "2503a9b4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'teacher', 'rocks', 'like', 'a', 'good', 'rock', '&', 'roll'],\n",
       " ['the', 'rock', 'is', 'the', 'best', 'actor', 'in', 'the', 'world'],\n",
       " ['New', 'York', 'is', 'a', 'beautiful', 'city']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ComparaciÃ³n con el caso anterior\n",
    "docs_tokenizados = [word_tokenize(doc) for doc in docs]\n",
    "docs_tokenizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "937e18f6c6a747f7893b43a25d6aa65c",
    "deepnote_cell_height": 110.78125,
    "deepnote_cell_type": "markdown",
    "id": "z58vnh-4jO0N",
    "tags": []
   },
   "source": [
    "#### Al Estilo Scikit\n",
    "\n",
    "Scikit implementa `bag of words` a travÃ©s de la clase `CountVectorizer()` la cual contiene muchas opciones para mejorar la tokenizaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "c898d0dca5bd47cd80f4a8566cf5cc13",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "deepnote_cell_height": 270,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     119
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 152,
    "execution_start": 1637346927803,
    "id": "lszo5-WYjO0N",
    "outputId": "70ec3383-0ae3-46e2-f0b2-87b155ac15b1",
    "source_hash": "2bc7124d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-be56da2c-cd78-4f5b-958b-5ed8b645eaaa\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&amp;</th>\n",
       "      <th>actor</th>\n",
       "      <th>beauti</th>\n",
       "      <th>best</th>\n",
       "      <th>citi</th>\n",
       "      <th>good</th>\n",
       "      <th>like</th>\n",
       "      <th>new</th>\n",
       "      <th>rock</th>\n",
       "      <th>roll</th>\n",
       "      <th>teacher</th>\n",
       "      <th>world</th>\n",
       "      <th>york</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be56da2c-cd78-4f5b-958b-5ed8b645eaaa')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-be56da2c-cd78-4f5b-958b-5ed8b645eaaa button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-be56da2c-cd78-4f5b-958b-5ed8b645eaaa');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   &  actor  beauti  best  citi  good  like  new  rock  roll  teacher  world  \\\n",
       "0  1      0       0     0     0     1     1    0     2     1        1      0   \n",
       "1  0      1       0     1     0     0     0    0     1     0        0      1   \n",
       "2  0      0       1     0     1     0     0    1     0     0        0      0   \n",
       "\n",
       "   york  \n",
       "0     0  \n",
       "1     0  \n",
       "2     1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer(tokenizer= StemmerTokenizer())\n",
    "df = bow.fit_transform(docs)\n",
    "\n",
    "pd.DataFrame(df.toarray(), columns=bow.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d9184cbee66a498bbe0050076342f306",
    "deepnote_cell_height": 155.953125,
    "deepnote_cell_type": "markdown",
    "id": "grqRuyUAjO0N",
    "tags": []
   },
   "source": [
    "Una de las cosas mÃ¡s interesantes que provee son el use de n-gramas, los cuales, en palabras simples, son conjuntos de n-palabras que se concatenan entre si y que se consideran como tokens separados. \n",
    "\n",
    "Pensemos en `Nueva York`. Cuando se tokeniza Nueva York, se generan dos tokens independientes que a simple vista no tienen relaciÃ³n: `Nueva` `York`.\n",
    "Al usar n-gramas (en un rango min=1,max=2) , generamos tanto `Nueva` y `York` como tambiÃ©n `Nueva York` como un token independiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "8aa91d98ce4d4fd1b336e730702a9832",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "deepnote_cell_height": 301.1875,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     150.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 241,
    "execution_start": 1637346930092,
    "id": "LDPiob_tjO0O",
    "outputId": "103e0569-0ba9-4224-ff83-5406f7b17d15",
    "source_hash": "6af25c7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c5b1aafe-4631-447f-bbd8-12f7ff399c2d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&amp;</th>\n",
       "      <th>&amp; roll</th>\n",
       "      <th>actor</th>\n",
       "      <th>actor world</th>\n",
       "      <th>beauti</th>\n",
       "      <th>beauti citi</th>\n",
       "      <th>best</th>\n",
       "      <th>best actor</th>\n",
       "      <th>citi</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>rock</th>\n",
       "      <th>rock &amp;</th>\n",
       "      <th>rock best</th>\n",
       "      <th>rock like</th>\n",
       "      <th>roll</th>\n",
       "      <th>teacher</th>\n",
       "      <th>teacher rock</th>\n",
       "      <th>world</th>\n",
       "      <th>york</th>\n",
       "      <th>york beauti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 25 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5b1aafe-4631-447f-bbd8-12f7ff399c2d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c5b1aafe-4631-447f-bbd8-12f7ff399c2d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c5b1aafe-4631-447f-bbd8-12f7ff399c2d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   &  & roll  actor  actor world  beauti  beauti citi  best  best actor  citi  \\\n",
       "0  1       1      0            0       0            0     0           0     0   \n",
       "1  0       0      1            1       0            0     1           1     0   \n",
       "2  0       0      0            0       1            1     0           0     1   \n",
       "\n",
       "   good  ...  rock  rock &  rock best  rock like  roll  teacher  teacher rock  \\\n",
       "0     1  ...     2       1          0          1     1        1             1   \n",
       "1     0  ...     1       0          1          0     0        0             0   \n",
       "2     0  ...     0       0          0          0     0        0             0   \n",
       "\n",
       "   world  york  york beauti  \n",
       "0      0     0            0  \n",
       "1      1     0            0  \n",
       "2      0     1            1  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer(tokenizer= StemmerTokenizer(), ngram_range=(1,2))\n",
    "df = bow.fit_transform(docs)\n",
    "\n",
    "pd.DataFrame(df.toarray(), columns=bow.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "81f6a40097954939a7854adbbe28fdc6",
    "deepnote_cell_height": 97.171875,
    "deepnote_cell_type": "markdown",
    "id": "ItkxkdVjjO0O",
    "tags": []
   },
   "source": [
    "De los resultados, podemos ver que generamos vectores de conteo para cada una de las palabras que conforman el corpus.  Un punto extra que se agrega en esta obtenciÃ³n de frecuencias son los bigramas, que bÃ¡sicamente son el conjunto de palabras de tamaÃ±o de aparecen juntas en el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "07780e17fbf349afaba37a0aef11e2f1",
    "deepnote_cell_height": 547,
    "deepnote_cell_type": "markdown",
    "id": "dmQtiVo6jO0O",
    "tags": []
   },
   "source": [
    "## Codificando los Super{heroes, villanos}  [0.5 Puntos]\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://c.tenor.com/LkQzw7k5DV4AAAAd/anime-hacking.gif\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "Conociendo ahora que es el proceso de `bag of words`, aplique este modelo de obtenciÃ³n de caracteristicas de la siguiente forma en un pipeline:\n",
    "\n",
    "- Utilice el tokenizador entregado.\n",
    "- Obtenga caracteristicas de los unigramas y bigramas del texto (tal como el ejemplo).\n",
    "\n",
    "```python\n",
    "bog = CountVectorizer(tokenizer= StemmerTokenizer(),`\n",
    "                      ngram_range=(1,2) # Este punto es opcional y es para generar bigramas\n",
    "                      )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "58abb21ee53c424a96ca09d3c3f91d51",
    "deepnote_cell_height": 332.90625,
    "deepnote_cell_type": "markdown",
    "id": "J2kGImsTjO0O",
    "tags": []
   },
   "source": [
    "Finalmente, aplique `MinMaxScaler()` sobre `atributos_de_interes` y concatene el valor obtenido con el matriz de caracteristicas obtenidas con bag of words.\n",
    "\n",
    "```python\n",
    "atributos_de_interes = ['intelligence_score', 'strength_score', 'speed_score', 'durability_score', 'power_score', 'combat_score']\n",
    "```\n",
    "\n",
    "No es necesario que obtenga un dataframe en concreto con las caracterÃ­sticas solicitadas. Se le recomienda generar un `ColumnTransformer()` para aplicar las transformaciones solicitadas en un pipeline.\n",
    "\n",
    "**To-Do:**\n",
    "- [ ] Obtener a traves de Bag of Words (`CountVectorizer`) caracteristicas del resumen de historia de cada personaje.\n",
    "- [ ] Aplicar `MinMaxScaler` sobre los atributos de interes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a33d10178fa84f7f8834eeaddf78f4c4",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "id": "1mCXeXkmjO0P",
    "tags": []
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "20d7a7fe8e024e408b2016a1e4d6deef",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "id": "ay080DunHcOS"
   },
   "outputs": [],
   "source": [
    "#### CÃ³digo aquÃ­ ####\n",
    "atributos_de_interes = [\n",
    "    'intelligence_score',\n",
    "    'strength_score',\n",
    "    'speed_score',\n",
    "    'durability_score',\n",
    "    'power_score',\n",
    "    'combat_score'\n",
    "    ]\n",
    "\n",
    "bow = CountVectorizer(\n",
    "    tokenizer = StemmerTokenizer(),\n",
    "    ngram_range = (1,2)\n",
    ")\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('CountVectorizer', bow, 'history_text'),\n",
    "        ('MinMaxScaler', MinMaxScaler(), atributos_de_interes)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9df292283e4449b2a4b5d60dcf2987c0",
    "deepnote_cell_height": 317.5,
    "deepnote_cell_type": "markdown",
    "id": "stHncQ-A-j4I",
    "owner_user_id": "d50c3174-babb-4861-9c71-7e3af66458b8"
   },
   "source": [
    "## 1.2 DiseÃ±o de Baseline y  Primer Entrenamiento  [1 Puntos]\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://pa1.narvii.com/6374/9eaec1b7bf9157334151452a669516f9a78b954c_hq.gif\" width=\"300\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0de6d1a9fc4d400f972e7d0511ce2cf3",
    "deepnote_cell_height": 455.859375,
    "deepnote_cell_type": "markdown",
    "id": "NeMiptpQ_EWb"
   },
   "source": [
    "\n",
    "Genere un Pipeline con las caracteristicas solicitadas en la secciÃ³n 1.1, un selector de mejores features `SelectPercentile` con mÃ©trica `f_classif` y percentile=90 y un clasificador `MultinomialNB()` por defecto.\n",
    "\n",
    "Luego, separe el conjunto de datos en un conjunto de entrenamiento y prueba, donde las etiquetas estarÃ¡ dado por el atributo `alignment`. \n",
    "\n",
    "Entrene el modelo y reporte el desempeÃ±o con un `classification_report`. Â¿ Nos recomendarÃ­a predecir la alineaciÃ³n de BatCow con este clasificador?.\n",
    "\n",
    "Finalmente, compare el modelo entrenado con un modelo Dummy estratificado y responda: Â¿El clasificador entrenado es mejor que el dummy que entrega respuestas al azar?\n",
    "\n",
    "**To-do:**\n",
    "- [ ] Realizar un pipeline con las caracteristicas solicitadas en 1.1, ejecutar holdout y aplicar un clasificador `MultinomialNB()`.\n",
    "- [ ] Entrenar el pipeline, calcular el `classification_report` asociado y comentar los resultados.\n",
    "- [ ] Entrenar un `DummyClassifier` con estrategia `statified`, calcular el `classification_report` asociado y comentar que implican los scores obtenidos en comparaciÃ³n con los resultados del baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "81b793138ec14f3d92713dab4e6bebb9",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "id": "oK61ytYsjO0P",
    "tags": []
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "e68a1a925b9d429d93f7b3e888eb9c06",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "id": "_hHpPDooPafy",
    "outputId": "10d926e1-f4f4-42e3-a20e-842f62d5bd55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.69      0.10      0.18        86\n",
      "        Good       0.59      0.98      0.74       148\n",
      "     Neutral       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.60       257\n",
      "   macro avg       0.43      0.36      0.31       257\n",
      "weighted avg       0.57      0.60      0.49       257\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.32      0.34      0.33        86\n",
      "        Good       0.58      0.56      0.57       148\n",
      "     Neutral       0.12      0.13      0.12        23\n",
      "\n",
      "    accuracy                           0.45       257\n",
      "   macro avg       0.34      0.34      0.34       257\n",
      "weighted avg       0.46      0.45      0.45       257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#### CÃ³digo aquÃ­ ####\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "super_identifier_pipe = Pipeline([\n",
    "    ('Preprocessing', transformer),\n",
    "    ('Feature selection', SelectPercentile(f_classif, percentile=90)),\n",
    "    ('Classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "X = df_comics.drop('alignment', axis=1)\n",
    "y = df_comics.alignment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "super_identifier_pipe.fit(X_train, y_train)\n",
    "y_pred = super_identifier_pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "dummy = DummyClassifier(strategy='stratified', random_state=42)\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dummy = dummy.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f69a4b33bd9442fabbc34bcaf2c87581",
    "deepnote_cell_height": 70.796875,
    "deepnote_cell_type": "markdown",
    "id": "x_-n20fNjO0Q",
    "tags": []
   },
   "source": [
    "El clasificador entrenado posee un muy buen *recall* para la clase *Good*, pero tambiÃ©n cuenta con casi un 60% para la mÃ©trica *precision*. Esto quiere decir que existe un sesgo hacia esta clase, pues el modelo asigna esta etiqueta tanto para las muestras que realmente pertenecen a esta clase, como las de otras clases. Como consecuencia, el modelo no ha retornado para ningÃºn ejemplo la clase *Neutral*, correspondiente a la clase con menos ejemplos. AdemÃ¡s, sÃ³lo el 10% de las muestras con clase *Bad* se le han asignado correctamente esa clase.\n",
    "\n",
    "El *DummyClassifier* si encuentra ejemplos de las clases *Neutral* y mÃ¡s ejemplos de la clase *Bad*, sin embargo, esto se debe a la forma en que se le ha pedido que haga las predicciones (stratified). Al comparar las *macro_avg* y las *weighted_avg* se puede notar que en practicamente todas el clasificador entrenado es mejor que el *DummyClassfier*, por lo que el primero es mÃ¡s confiable que el Ãºltimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "efd50fb630984a12a0a56ced0c73e088",
    "deepnote_cell_height": 400,
    "deepnote_cell_type": "markdown",
    "id": "pfm7I2B7_rfB"
   },
   "source": [
    "## 1.3 Busqueda del Mejor Modelo con Grid Search [4 Puntos]\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/images/70fdfeea52a8e2e4505498c230a0d2f9/tenor.gif?itemid=5134219\" width=\"250\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "69063d71deb042109162f3cb4199b231",
    "deepnote_cell_height": 859.5,
    "deepnote_cell_type": "markdown",
    "id": "14siiavzK67p"
   },
   "source": [
    "No conformes con el rendimiento obtenido en la secciÃ³n 1.2, el cuerpo docente les pide que realicen un **`HalvingGridSearchCV`** con diferentes parÃ¡metros para mejorar el rendimiento de la clasificaciÃ³n. Para esto, se le solicita que defina:\n",
    "\n",
    "- Tres clasificadores distintos en donde varie sus parÃ¡metros. Considere usar modelos clÃ¡sicos como tambiÃ©n los basados en ensamblaje.\n",
    "- Modificar `n-gram` range del `CountVectorizer` probando `(1,1), (1,2) y (1,3)`. Examinar tambiÃ©n los otros parÃ¡metros de CountVectorizer como por ejemplo `max_df`, `min_df`, etc... ([DocumentaciÃ³n aquÃ­](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html))\n",
    "- Seleccionar las columnas que contribuyen con la mayor informaciÃ³n para la clasificaciÃ³n con `SelectPercentile` en los percentiles `[20, 40, 60, 80]` (puede usar la mÃ©trica que usted quiera).\n",
    "- Reporte la mejor combinaciÃ³n encontrada y justifique por quÃ© cree que es la mejor segÃºn el clasificador usado, la cantidad de columnas seleccionadas y los parÃ¡metros de CountVectorizer seleccionados por GridSearch.\n",
    "\n",
    "A continuaciÃ³n, un ejemplo de parametros para GridSearch para una bÃºsqueda de 3 clasificadores distintos:\n",
    "\n",
    "```python\n",
    "params = [\n",
    "       # clasificador 1 + hiperparÃ¡metros\n",
    "       {'clf': classificator1(),\n",
    "        'clf__penalty': ['ovr'],\n",
    "       # clasificador 1 + hiperparÃ¡metros    \n",
    "       {'clf': classificator2(),\n",
    "        'clf__n_estimators': [200]},\n",
    "       # clasificador 1 + hiperparÃ¡metros\n",
    "       {'clf': classificator3(),\n",
    "        ...\n",
    "       }\n",
    "       ]\n",
    "```\n",
    "\n",
    "**Nota 1**: Puede ver los parÃ¡metros modificables aplicando el mÃ©todo get_params() sobre su pipeline. Ver la clase de GridSearch para mayor informaciÃ³n sobre la sintÃ¡xis de las grillas.\n",
    "\n",
    "**Nota 2**: Recuerde inicializar los clasificadores con un random state definido.\n",
    "\n",
    "**Nota 3**: Puede usar en `HalvingGridSearchCV` el parÃ¡metro `verbose=10` para ver que GridSearch le indique el estado de su ejecuciÃ³n.\n",
    "\n",
    "**Nota 3:** El GridSearch puede tomar tiempos de bÃºsqueda exorbitantes, por lo que se le recomienda no agrandar mucho el espacio de bÃºsqueda, dejar corriendo el cÃ³digo y tomarse un tecito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7c9f3de702234ddca989bf2125fab779",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "id": "HlYtgnI7jO0Q",
    "tags": []
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cell_id": "4663cec8ef58413cb9cf36044d5c8da2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "id": "oNvHOHELUoIv",
    "outputId": "4f045f4d-ff74-4901-b110-8762f21c7792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 18\n",
      "max_resources_: 1028\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 240\n",
      "n_resources: 18\n",
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n",
      "[CV 1/3; 1/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 1/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 1/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 1/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 1/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 1/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.417, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 2/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 2/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 2/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 2/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 2/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 2/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.417, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 3/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 3/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 3/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 3/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 3/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 3/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.417, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 4/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 4/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 4/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 4/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 4/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 4/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 5/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 5/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 5/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 5/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 5/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 5/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 6/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 6/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 6/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 6/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 6/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 6/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 7/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 7/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 7/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 7/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 7/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 7/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 8/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 8/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 8/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 8/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 8/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 8/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 9/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 9/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 9/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 9/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 9/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 9/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.417, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 10/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 10/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 10/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 10/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 10/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 10/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 11/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 11/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 11/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 11/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 11/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 11/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 12/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 12/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 12/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 12/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 12/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 12/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 13/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 13/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 13/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 13/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 13/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 13/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 14/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 14/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 14/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 14/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 14/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 14/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 15/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 15/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 15/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 15/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 15/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 15/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.417, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 16/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 16/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 16/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 16/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 16/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 16/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 17/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 17/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 17/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 17/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 17/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 17/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 18/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 18/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 18/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 18/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 18/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 18/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.417, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 19/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 19/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 19/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 19/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 19/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 19/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 20/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 20/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 20/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 20/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 20/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 20/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.417, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 21/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 21/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 21/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 21/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 21/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 21/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.417, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 22/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 22/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 22/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 22/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 22/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 22/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 23/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 23/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 23/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 23/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 23/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 23/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 24/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 24/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 24/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 24/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 24/240] START Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 24/240] END Classifier=SVC(), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 25/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 25/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.818, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 25/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 25/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 25/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 25/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.500, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 26/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 26/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 26/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 26/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 26/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 26/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.583, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 27/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 27/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 27/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 27/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 27/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 27/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.583, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 28/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 28/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 28/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 28/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.909, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 28/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 28/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.750, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 29/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 29/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 29/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 29/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 29/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 29/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.750, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 30/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 30/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 30/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 30/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 30/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 30/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.750, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 31/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 31/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.818, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 31/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 31/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 31/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 31/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.667, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 32/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 32/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 2/3; 32/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 32/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 32/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 32/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.583, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 33/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 33/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.818, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 33/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 33/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 33/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 33/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.667, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 34/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 34/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.818, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 34/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 34/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.818, test=0.667) total time=   0.2s\n",
      "[CV 3/3; 34/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 34/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.750, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 35/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 35/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.818, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 35/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 35/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.818, test=0.667) total time=   0.2s\n",
      "[CV 3/3; 35/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 35/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.750, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 36/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 36/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.818, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 36/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 36/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.818, test=0.667) total time=   0.2s\n",
      "[CV 3/3; 36/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 36/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.750, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 37/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 37/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.727, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 37/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 37/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 37/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 37/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.500, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 38/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 38/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.818, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 38/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 38/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 38/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 38/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.667, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 39/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 39/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.818, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 39/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 39/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 39/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 39/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.667, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 40/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 40/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 40/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 40/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 40/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 40/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 41/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 41/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 41/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 41/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 41/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 41/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 42/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 42/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 42/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 42/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 42/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 42/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 43/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 43/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.727, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 43/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 43/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 43/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 43/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.500, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 44/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 44/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.818, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 44/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 44/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 44/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 44/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.500, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 45/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 45/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.818, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 45/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 45/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 45/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 45/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.500, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 46/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 46/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 46/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 46/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 46/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 46/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.500, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 47/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 47/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 47/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 47/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 47/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 47/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.500, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 48/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 48/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 48/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 48/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 48/240] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 48/240] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.500, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 49/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 49/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 49/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 49/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.000) total time=   0.2s\n",
      "[CV 3/3; 49/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 49/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.917, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 50/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 50/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 50/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 50/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.2s\n",
      "[CV 3/3; 50/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 50/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.917, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 51/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 51/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 51/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 51/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 51/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 51/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.917, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 52/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 52/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 52/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 52/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.667) total time=   0.2s\n",
      "[CV 3/3; 52/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 52/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 53/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 53/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 53/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 53/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.667) total time=   0.2s\n",
      "[CV 3/3; 53/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 53/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 54/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 54/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 54/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 54/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.667) total time=   0.2s\n",
      "[CV 3/3; 54/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 54/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 55/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 55/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 55/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 55/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 55/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 55/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.917, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 56/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 56/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 56/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 56/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 56/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 56/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.917, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 57/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 57/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 57/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 57/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 57/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 57/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.917, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 58/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 58/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 58/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 58/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.909, test=0.667) total time=   0.2s\n",
      "[CV 3/3; 58/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 58/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.917, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 59/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 59/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 59/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 59/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.667) total time=   0.2s\n",
      "[CV 3/3; 59/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 59/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.917, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 60/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 60/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 60/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 60/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.667) total time=   0.2s\n",
      "[CV 3/3; 60/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 60/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.917, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 61/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 61/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 61/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 61/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.909, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 61/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 61/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.750, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 62/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 62/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.2s\n",
      "[CV 2/3; 62/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 62/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 62/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 62/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.917, test=0.600) total time=   0.8s\n",
      "[CV 1/3; 63/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 63/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.167) total time=   0.2s\n",
      "[CV 2/3; 63/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 63/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 63/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 63/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.917, test=0.600) total time=   0.9s\n",
      "[CV 1/3; 64/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 64/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 2/3; 64/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 64/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.727, test=0.833) total time=   0.3s\n",
      "[CV 3/3; 64/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 64/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.667, test=0.800) total time=   0.7s\n",
      "[CV 1/3; 65/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 65/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 2/3; 65/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 65/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.727, test=0.833) total time=   0.3s\n",
      "[CV 3/3; 65/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 65/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.750, test=0.800) total time=   0.7s\n",
      "[CV 1/3; 66/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 66/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 2/3; 66/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 66/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.727, test=0.833) total time=   0.3s\n",
      "[CV 3/3; 66/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 66/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.750, test=0.800) total time=   0.8s\n",
      "[CV 1/3; 67/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 67/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.167) total time=   0.2s\n",
      "[CV 2/3; 67/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 67/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.909, test=0.667) total time=   0.3s\n",
      "[CV 3/3; 67/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 67/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.750, test=0.800) total time=   0.7s\n",
      "[CV 1/3; 68/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 68/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 2/3; 68/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 68/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.667) total time=   0.3s\n",
      "[CV 3/3; 68/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 68/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.917, test=0.800) total time=   0.6s\n",
      "[CV 1/3; 69/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 69/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 69/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 69/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 69/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 69/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.917, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 70/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 70/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.909, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 70/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 70/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.727, test=0.833) total time=   0.2s\n",
      "[CV 3/3; 70/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 70/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.750, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 71/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 71/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 71/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 71/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.818, test=0.833) total time=   0.2s\n",
      "[CV 3/3; 71/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 71/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.750, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 72/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 72/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 72/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 72/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.818, test=0.833) total time=   0.2s\n",
      "[CV 3/3; 72/240] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 72/240] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.750, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 73/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 73/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 73/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 73/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 73/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 73/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 74/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 74/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 74/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 74/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 74/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 74/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 75/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 75/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 75/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 75/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 75/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 75/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 76/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 76/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 2/3; 76/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 76/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 76/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 76/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 77/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 77/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 77/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 77/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 77/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 77/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 78/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 78/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 78/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 78/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 78/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 78/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 79/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 79/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 79/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 79/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 79/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 79/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 80/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 80/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 80/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 80/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 80/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 80/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 81/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 81/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 81/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 81/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 81/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 81/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 82/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 82/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 82/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 82/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 82/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 82/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 83/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 83/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 2/3; 83/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 83/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 83/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 83/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.800) total time=   0.6s\n",
      "[CV 1/3; 84/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 84/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 84/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 84/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.4s\n",
      "[CV 3/3; 84/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 84/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 85/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 85/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 85/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 85/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 85/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 85/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   0.6s\n",
      "[CV 1/3; 86/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 86/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 86/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 86/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 86/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 86/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 87/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 87/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 87/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 87/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 87/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 87/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.7s\n",
      "[CV 1/3; 88/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 88/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 88/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 88/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.4s\n",
      "[CV 3/3; 88/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 88/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 89/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 89/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 89/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 89/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.3s\n",
      "[CV 3/3; 89/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 89/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 90/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 90/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 90/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 90/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.3s\n",
      "[CV 3/3; 90/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 90/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.800) total time=   0.6s\n",
      "[CV 1/3; 91/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 91/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 91/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 91/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 91/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 91/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 92/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 92/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 92/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 92/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 92/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 92/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 93/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 93/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 93/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 93/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 93/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 93/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 94/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 94/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 2/3; 94/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 94/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.667) total time=   0.4s\n",
      "[CV 3/3; 94/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 94/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 95/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 95/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 95/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 95/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.3s\n",
      "[CV 3/3; 95/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 95/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 96/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 96/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 96/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 96/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.3s\n",
      "[CV 3/3; 96/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 96/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 97/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 97/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 97/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 97/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   1.0s\n",
      "[CV 3/3; 97/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 97/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 98/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 98/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 98/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 98/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 98/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 98/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.4s\n",
      "[CV 1/3; 99/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 99/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 99/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 99/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   1.0s\n",
      "[CV 3/3; 99/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 99/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.3s\n",
      "[CV 1/3; 100/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 100/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 100/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 100/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 100/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 100/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 101/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 101/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 101/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 101/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 101/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 101/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 102/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 102/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 102/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 102/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 102/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 102/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 103/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 103/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 103/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 103/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 103/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 103/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 104/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 104/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 104/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 104/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 104/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 104/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 105/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 105/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 105/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 105/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 105/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 105/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 106/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 106/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 106/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 106/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.8s\n",
      "[CV 3/3; 106/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 106/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 107/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 107/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 107/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 107/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 107/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 107/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.800) total time=   1.2s\n",
      "[CV 1/3; 108/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 108/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 108/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 108/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 108/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 108/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 109/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 109/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 109/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 109/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 109/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 109/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   1.1s\n",
      "[CV 1/3; 110/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 110/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 110/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 110/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 110/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 110/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 111/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 111/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 111/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 111/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 111/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 111/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.4s\n",
      "[CV 1/3; 112/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 112/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   1.0s\n",
      "[CV 2/3; 112/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 112/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 112/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 112/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   1.4s\n",
      "[CV 1/3; 113/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 113/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   1.0s\n",
      "[CV 2/3; 113/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 113/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 113/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 113/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 114/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 114/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 114/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 114/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 114/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 114/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.800) total time=   1.2s\n",
      "[CV 1/3; 115/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 115/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 115/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 115/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 115/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 115/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 116/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 116/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 116/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 116/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 116/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 116/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 117/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 117/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 117/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 117/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 117/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 117/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.3s\n",
      "[CV 1/3; 118/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 118/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 118/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 118/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 118/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 118/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   1.2s\n",
      "[CV 1/3; 119/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 119/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 119/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 119/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 119/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 119/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 120/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 120/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 120/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 120/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 120/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 120/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 121/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 121/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 121/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 121/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 121/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 121/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 122/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 122/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 122/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 122/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 122/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 122/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.7s\n",
      "[CV 1/3; 123/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 123/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 123/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 123/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 123/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 123/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.7s\n",
      "[CV 1/3; 124/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 124/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 124/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 124/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 124/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 124/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 125/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 125/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 125/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 125/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.4s\n",
      "[CV 3/3; 125/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 125/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 126/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 126/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 126/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 126/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 126/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 126/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.7s\n",
      "[CV 1/3; 127/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 127/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 127/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 127/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 127/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 127/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 128/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 128/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 128/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 128/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 128/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 128/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 129/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 129/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 129/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 129/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 129/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 129/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.7s\n",
      "[CV 1/3; 130/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 130/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 130/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 130/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.3s\n",
      "[CV 3/3; 130/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 130/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 131/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 131/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 131/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 131/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.3s\n",
      "[CV 3/3; 131/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 131/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.800) total time=   0.6s\n",
      "[CV 1/3; 132/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 132/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 132/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 132/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.667) total time=   0.3s\n",
      "[CV 3/3; 132/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 132/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.800) total time=   0.6s\n",
      "[CV 1/3; 133/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 133/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 133/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 133/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 133/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 133/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   0.6s\n",
      "[CV 1/3; 134/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 134/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 134/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 134/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 134/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 134/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 135/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 135/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 135/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 135/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 135/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 135/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 136/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 136/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 136/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 136/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.3s\n",
      "[CV 3/3; 136/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 136/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 137/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 137/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 137/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 137/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.3s\n",
      "[CV 3/3; 137/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 137/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 138/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 138/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 138/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 138/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.4s\n",
      "[CV 3/3; 138/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 138/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 139/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 139/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 139/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 139/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 139/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 139/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 140/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 140/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 140/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 140/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 140/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 140/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 141/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 141/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 141/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 141/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.4s\n",
      "[CV 3/3; 141/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 141/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.7s\n",
      "[CV 1/3; 142/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 142/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 142/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 142/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.667) total time=   0.3s\n",
      "[CV 3/3; 142/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 142/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 143/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 143/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 2/3; 143/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 143/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.3s\n",
      "[CV 3/3; 143/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 143/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 144/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 144/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 2/3; 144/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 144/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.3s\n",
      "[CV 3/3; 144/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 144/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 145/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 145/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 145/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 145/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 145/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 145/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 146/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 146/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   1.0s\n",
      "[CV 2/3; 146/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 146/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   1.1s\n",
      "[CV 3/3; 146/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 146/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 147/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 147/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 147/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 147/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 147/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 147/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.7s\n",
      "[CV 1/3; 148/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 148/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   1.4s\n",
      "[CV 2/3; 148/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 148/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.5s\n",
      "[CV 3/3; 148/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 148/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.9s\n",
      "[CV 1/3; 149/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 149/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   1.3s\n",
      "[CV 2/3; 149/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 149/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   1.5s\n",
      "[CV 3/3; 149/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 149/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.9s\n",
      "[CV 1/3; 150/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 150/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   1.2s\n",
      "[CV 2/3; 150/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 150/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.2s\n",
      "[CV 3/3; 150/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 150/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 151/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 151/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 151/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 151/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 151/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 151/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 152/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 152/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 152/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 152/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 152/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 152/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 153/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 153/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 153/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 153/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 153/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 153/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 154/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 154/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 154/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 154/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.667) total time=   0.9s\n",
      "[CV 3/3; 154/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 154/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 155/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 155/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 155/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 155/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 155/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 155/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.800) total time=   1.2s\n",
      "[CV 1/3; 156/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 156/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 156/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 156/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.667) total time=   0.9s\n",
      "[CV 3/3; 156/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 156/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 157/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 157/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 157/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 157/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 157/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 157/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   1.1s\n",
      "[CV 1/3; 158/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 158/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 158/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 158/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 158/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 158/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 159/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 159/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 159/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 159/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 159/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 159/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 160/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 160/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 160/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 160/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 160/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 160/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 161/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 161/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 161/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 161/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 161/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 161/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 162/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 162/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 162/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 162/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 162/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 162/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.800) total time=   1.2s\n",
      "[CV 1/3; 163/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 163/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 163/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 163/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 163/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 163/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   1.1s\n",
      "[CV 1/3; 164/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 164/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 164/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 164/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 3/3; 164/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 164/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.4s\n",
      "[CV 1/3; 165/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 165/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 165/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 165/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   1.0s\n",
      "[CV 3/3; 165/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 165/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.4s\n",
      "[CV 1/3; 166/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 166/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 166/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 166/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 166/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 166/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   1.2s\n",
      "[CV 1/3; 167/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 167/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.9s\n",
      "[CV 2/3; 167/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 167/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 167/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 167/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 168/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 168/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.8s\n",
      "[CV 2/3; 168/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 168/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 168/240] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 168/240] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   1.2s\n",
      "[CV 1/3; 169/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 169/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 169/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 169/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.636, test=0.167) total time=   0.2s\n",
      "[CV 3/3; 169/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 169/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.750, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 170/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 170/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 170/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 170/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.636, test=0.667) total time=   0.2s\n",
      "[CV 3/3; 170/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 170/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.750, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 171/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 171/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 171/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 171/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 171/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 171/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.750, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 172/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 172/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 172/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 172/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 172/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 172/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 173/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 173/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 173/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 173/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 173/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 173/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.417, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 174/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 174/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 174/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 174/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 174/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 174/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.417, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 175/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 175/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 175/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 175/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 175/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 175/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.833, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 176/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 176/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 176/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 176/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 176/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 176/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.917, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 177/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 177/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 177/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 177/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 177/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 177/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.750, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 178/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 178/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 178/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 178/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 178/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 178/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.667, test=0.400) total time=   0.4s\n",
      "[CV 1/3; 179/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 179/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 179/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 179/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 179/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 179/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.667, test=0.400) total time=   0.4s\n",
      "[CV 1/3; 180/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 180/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 180/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 180/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 180/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 180/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.667, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 181/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 181/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 181/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 181/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 181/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 181/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.833, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 182/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 182/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 182/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 182/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 182/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 182/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.917, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 183/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 183/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 183/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 183/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 183/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 183/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.917, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 184/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 184/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 184/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 184/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 184/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 184/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.667, test=0.400) total time=   0.4s\n",
      "[CV 1/3; 185/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 185/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 185/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 185/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 185/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 185/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.667, test=0.400) total time=   0.4s\n",
      "[CV 1/3; 186/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 186/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 186/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 186/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 186/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 186/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.667, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 187/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 187/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 187/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 187/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 187/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 187/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.833, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 188/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 188/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 188/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 188/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 188/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 188/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.833, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 189/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 189/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 189/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 189/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 189/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 189/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.833, test=0.400) total time=   0.6s\n",
      "[CV 1/3; 190/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 190/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 190/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 190/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 190/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 190/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.667, test=0.400) total time=   0.4s\n",
      "[CV 1/3; 191/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 191/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 191/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 191/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 191/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 191/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.667, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 192/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 192/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 192/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 192/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.545, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 192/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 192/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.667, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 193/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 193/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 193/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 193/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 193/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 193/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 194/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 194/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 194/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 194/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.2s\n",
      "[CV 3/3; 194/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 194/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 195/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 195/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 195/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 195/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 195/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 195/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 196/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 196/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 196/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 196/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 196/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 196/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 197/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 197/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 197/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 197/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 197/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 197/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 198/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 198/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 198/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 198/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 198/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 198/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 199/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 199/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 199/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 199/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 199/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 199/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 200/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 200/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 200/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 200/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 200/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 200/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 201/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 201/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 201/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 201/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 201/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 201/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.800) total time=   0.6s\n",
      "[CV 1/3; 202/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 202/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 202/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 202/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 202/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 202/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 203/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 203/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 203/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 203/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 203/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 203/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 204/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 204/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 204/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 204/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 204/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 204/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 205/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 205/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 205/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 205/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 205/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 205/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 206/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 206/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 206/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 206/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 206/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 206/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 207/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 207/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 207/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 207/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 207/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 207/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 208/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 208/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 208/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 208/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.636, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 208/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 208/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.917, test=0.400) total time=   0.4s\n",
      "[CV 1/3; 209/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 209/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 209/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 209/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.636, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 209/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 209/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.400) total time=   0.4s\n",
      "[CV 1/3; 210/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 210/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 210/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 210/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.636, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 210/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 210/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 211/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 211/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 211/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 211/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 211/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 211/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 212/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 212/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 212/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 212/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 212/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 212/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 213/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 213/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 213/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 213/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.909, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 213/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 213/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 214/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 214/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 214/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 214/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.636, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 214/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 214/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.917, test=0.400) total time=   0.4s\n",
      "[CV 1/3; 215/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 215/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 215/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 215/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.636, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 215/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 215/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 216/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 216/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 216/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 216/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.636, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 216/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 216/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 217/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 217/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 217/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 217/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.167) total time=   0.2s\n",
      "[CV 3/3; 217/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 217/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 218/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 218/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 218/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 218/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.2s\n",
      "[CV 3/3; 218/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 218/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 219/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 219/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 219/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 219/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 219/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 219/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.800) total time=   0.6s\n",
      "[CV 1/3; 220/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 220/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 220/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 220/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 220/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 220/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 221/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 221/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 221/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 221/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 221/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 221/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.800) total time=   0.4s\n",
      "[CV 1/3; 222/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 222/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.167) total time=   0.1s\n",
      "[CV 2/3; 222/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 222/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.2s\n",
      "[CV 3/3; 222/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 222/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 223/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 223/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 223/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 223/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 223/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 223/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 224/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 224/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 224/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 224/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 224/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 224/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.800) total time=   0.5s\n",
      "[CV 1/3; 225/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 225/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 225/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 225/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 225/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 225/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.800) total time=   0.6s\n",
      "[CV 1/3; 226/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 226/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 226/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 226/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 226/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 226/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.4s\n",
      "[CV 1/3; 227/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 227/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 227/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 227/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 227/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 227/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 228/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 228/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 228/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 228/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 228/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 228/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 229/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 229/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 229/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 229/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 229/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 229/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 230/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 230/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 230/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 230/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 230/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 230/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 231/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 231/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 231/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 231/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 231/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 231/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.7s\n",
      "[CV 1/3; 232/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 232/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 232/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 232/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.727, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 232/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 232/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 233/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 233/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 233/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 233/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 233/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 233/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 234/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 234/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 234/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 234/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 234/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 234/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 235/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 235/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 235/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 235/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.909, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 235/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 235/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.600) total time=   0.5s\n",
      "[CV 1/3; 236/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 236/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 236/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 236/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 236/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 236/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.600) total time=   0.6s\n",
      "[CV 1/3; 237/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 237/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 237/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 237/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.333) total time=   0.3s\n",
      "[CV 3/3; 237/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 237/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.600) total time=   0.8s\n",
      "[CV 1/3; 238/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 238/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.333) total time=   0.1s\n",
      "[CV 2/3; 238/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 238/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 238/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 238/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.400) total time=   0.4s\n",
      "[CV 1/3; 239/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 239/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 239/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 239/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 239/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 239/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.400) total time=   0.5s\n",
      "[CV 1/3; 240/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 240/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   0.1s\n",
      "[CV 2/3; 240/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 240/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.818, test=0.333) total time=   0.2s\n",
      "[CV 3/3; 240/240] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 240/240] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.400) total time=   0.5s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 80\n",
      "n_resources: 54\n",
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n",
      "[CV 1/3; 1/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 1/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.686, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 1/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 1/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.714, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 1/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 1/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.972, test=0.647) total time=   1.2s\n",
      "[CV 1/3; 2/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 2/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.389) total time=   2.4s\n",
      "[CV 2/3; 2/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 2/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 2/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 2/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   2.1s\n",
      "[CV 1/3; 3/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 3/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 3/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 3/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.743, test=0.444) total time=   1.0s\n",
      "[CV 3/3; 3/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 3/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.972, test=0.706) total time=   1.3s\n",
      "[CV 1/3; 4/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 4/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.389) total time=   2.9s\n",
      "[CV 2/3; 4/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 4/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.714, test=0.444) total time=   1.7s\n",
      "[CV 3/3; 4/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 4/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.972, test=0.706) total time=   2.2s\n",
      "[CV 1/3; 5/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 5/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.971, test=0.389) total time=   3.7s\n",
      "[CV 2/3; 5/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 5/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 5/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 5/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   2.1s\n",
      "[CV 1/3; 6/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 6/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.389) total time=   2.5s\n",
      "[CV 2/3; 6/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 6/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.8s\n",
      "[CV 3/3; 6/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 6/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   2.0s\n",
      "[CV 1/3; 7/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 7/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.971, test=0.389) total time=   1.8s\n",
      "[CV 2/3; 7/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 7/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 7/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 7/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   1.4s\n",
      "[CV 1/3; 8/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 8/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.971, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 8/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 8/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 8/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 8/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   1.4s\n",
      "[CV 1/3; 9/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 9/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.389) total time=   2.5s\n",
      "[CV 2/3; 9/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 9/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 9/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 9/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   1.9s\n",
      "[CV 1/3; 10/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 10/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.389) total time=   2.2s\n",
      "[CV 2/3; 10/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 10/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.6s\n",
      "[CV 3/3; 10/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 10/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   1.9s\n",
      "[CV 1/3; 11/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 11/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.971, test=0.389) total time=   2.3s\n",
      "[CV 2/3; 11/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 11/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.971, test=0.500) total time=   1.6s\n",
      "[CV 3/3; 11/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 11/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.588) total time=   1.8s\n",
      "[CV 1/3; 12/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 12/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.971, test=0.389) total time=   1.8s\n",
      "[CV 2/3; 12/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 12/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 12/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 12/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   1.4s\n",
      "[CV 1/3; 13/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 13/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.389) total time=   1.8s\n",
      "[CV 2/3; 13/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 13/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 13/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 13/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   1.3s\n",
      "[CV 1/3; 14/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 14/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.971, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 14/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 14/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 14/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 14/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   1.3s\n",
      "[CV 1/3; 15/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 15/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.971, test=0.389) total time=   2.4s\n",
      "[CV 2/3; 15/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 15/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 15/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 15/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   2.0s\n",
      "[CV 1/3; 16/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 16/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 16/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 16/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 16/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 16/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   1.4s\n",
      "[CV 1/3; 17/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 17/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.389) total time=   1.8s\n",
      "[CV 2/3; 17/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 17/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 17/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 17/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   1.4s\n",
      "[CV 1/3; 18/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 18/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.389) total time=   1.8s\n",
      "[CV 2/3; 18/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 18/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.2s\n",
      "[CV 3/3; 18/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 18/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   1.5s\n",
      "[CV 1/3; 19/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 19/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.971, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 19/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 19/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 19/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 19/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   1.2s\n",
      "[CV 1/3; 20/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 20/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.389) total time=   2.3s\n",
      "[CV 2/3; 20/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 20/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 20/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 20/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   2.0s\n",
      "[CV 1/3; 21/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 21/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.389) total time=   1.8s\n",
      "[CV 2/3; 21/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 21/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 21/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 21/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   1.3s\n",
      "[CV 1/3; 22/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 22/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 22/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 22/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 22/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 22/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.588) total time=   1.3s\n",
      "[CV 1/3; 23/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 23/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.389) total time=   2.5s\n",
      "[CV 2/3; 23/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 23/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 23/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 23/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   2.0s\n",
      "[CV 1/3; 24/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 24/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.971, test=0.389) total time=   2.2s\n",
      "[CV 2/3; 24/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 24/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.6s\n",
      "[CV 3/3; 24/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 24/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   1.9s\n",
      "[CV 1/3; 25/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 25/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.389) total time=   2.4s\n",
      "[CV 2/3; 25/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 25/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.8s\n",
      "[CV 3/3; 25/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 25/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   2.0s\n",
      "[CV 1/3; 26/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 26/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   2.6s\n",
      "[CV 2/3; 26/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 26/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.743, test=0.500) total time=   1.4s\n",
      "[CV 3/3; 26/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 26/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.972, test=0.588) total time=   1.7s\n",
      "[CV 1/3; 27/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 27/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.389) total time=   2.3s\n",
      "[CV 2/3; 27/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 27/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 27/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 27/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   2.0s\n",
      "[CV 1/3; 28/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 28/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   2.9s\n",
      "[CV 2/3; 28/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 28/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.886, test=0.500) total time=   2.7s\n",
      "[CV 3/3; 28/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 28/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   3.2s\n",
      "[CV 1/3; 29/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 29/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.389) total time=   4.0s\n",
      "[CV 2/3; 29/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 29/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 29/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 29/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   2.0s\n",
      "[CV 1/3; 30/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 30/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 30/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 30/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 30/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 30/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   1.5s\n",
      "[CV 1/3; 31/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 31/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.943, test=0.389) total time=   1.8s\n",
      "[CV 2/3; 31/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 31/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.743, test=0.444) total time=   1.1s\n",
      "[CV 3/3; 31/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 31/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.972, test=0.647) total time=   1.8s\n",
      "[CV 1/3; 32/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 32/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.971, test=0.389) total time=   2.1s\n",
      "[CV 2/3; 32/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 32/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.5s\n",
      "[CV 3/3; 32/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 32/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   1.6s\n",
      "[CV 1/3; 33/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 33/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.943, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 33/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 33/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.743, test=0.444) total time=   1.0s\n",
      "[CV 3/3; 33/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 33/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.972, test=0.647) total time=   1.2s\n",
      "[CV 1/3; 34/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 34/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.914, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 34/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 34/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.714, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 34/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 34/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.972, test=0.588) total time=   1.2s\n",
      "[CV 1/3; 35/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 35/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.389) total time=   2.0s\n",
      "[CV 2/3; 35/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 35/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.743, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 35/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 35/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.972, test=0.588) total time=   1.4s\n",
      "[CV 1/3; 36/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 36/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   2.6s\n",
      "[CV 2/3; 36/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 36/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.800, test=0.500) total time=   1.2s\n",
      "[CV 3/3; 36/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 36/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.917, test=0.529) total time=   1.7s\n",
      "[CV 1/3; 37/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 37/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.686, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 37/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 37/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.743, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 37/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 37/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.972, test=0.588) total time=   1.2s\n",
      "[CV 1/3; 38/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 38/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.389) total time=   2.2s\n",
      "[CV 2/3; 38/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 38/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.771, test=0.500) total time=   1.3s\n",
      "[CV 3/3; 38/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 38/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   1.6s\n",
      "[CV 1/3; 39/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 39/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.943, test=0.389) total time=   1.8s\n",
      "[CV 2/3; 39/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 39/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.714, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 39/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 39/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.972, test=0.647) total time=   1.3s\n",
      "[CV 1/3; 40/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 40/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.943, test=0.389) total time=   1.8s\n",
      "[CV 2/3; 40/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 40/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.714, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 40/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 40/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.972, test=0.647) total time=   1.2s\n",
      "[CV 1/3; 41/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 41/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   2.3s\n",
      "[CV 2/3; 41/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 41/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.771, test=0.500) total time=   1.2s\n",
      "[CV 3/3; 41/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 41/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.861, test=0.412) total time=   1.5s\n",
      "[CV 1/3; 42/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 42/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   2.2s\n",
      "[CV 2/3; 42/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 42/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.714, test=0.500) total time=   1.2s\n",
      "[CV 3/3; 42/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 42/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.778, test=0.588) total time=   1.7s\n",
      "[CV 1/3; 43/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 43/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.943, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 43/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 43/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.829, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 43/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 43/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.833, test=0.412) total time=   1.1s\n",
      "[CV 1/3; 44/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 44/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   2.0s\n",
      "[CV 2/3; 44/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 44/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.714, test=0.444) total time=   1.2s\n",
      "[CV 3/3; 44/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 44/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.722, test=0.471) total time=   1.6s\n",
      "[CV 1/3; 45/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 45/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.389) total time=   2.2s\n",
      "[CV 2/3; 45/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 45/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 45/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 45/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   1.9s\n",
      "[CV 1/3; 46/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 46/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.389) total time=   2.4s\n",
      "[CV 2/3; 46/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 46/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 46/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 46/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   2.1s\n",
      "[CV 1/3; 47/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 47/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 47/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 47/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 47/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 47/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.667, test=0.529) total time=   1.2s\n",
      "[CV 1/3; 48/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 48/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.971, test=0.389) total time=   2.2s\n",
      "[CV 2/3; 48/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 48/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 48/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 48/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   1.9s\n",
      "[CV 1/3; 49/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 49/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 49/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 49/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 49/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 49/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.588) total time=   1.3s\n",
      "[CV 1/3; 50/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 50/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.657, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 50/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 50/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 50/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 50/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.639, test=0.529) total time=   1.2s\n",
      "[CV 1/3; 51/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 51/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.657, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 51/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 51/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.686, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 51/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 51/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.639, test=0.529) total time=   1.1s\n",
      "[CV 1/3; 52/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 52/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 52/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 52/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 52/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 52/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.588) total time=   1.3s\n",
      "[CV 1/3; 53/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 53/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.686, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 53/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 53/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.686, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 53/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 53/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.694, test=0.529) total time=   1.1s\n",
      "[CV 1/3; 54/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 54/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 54/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 54/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 54/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 54/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.667, test=0.529) total time=   2.4s\n",
      "[CV 1/3; 55/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 55/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.657, test=0.389) total time=   3.0s\n",
      "[CV 2/3; 55/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 55/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 55/80] START Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 55/80] END Classifier=SVC(), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.639, test=0.529) total time=   1.7s\n",
      "[CV 1/3; 56/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 56/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.971, test=0.389) total time=   2.4s\n",
      "[CV 2/3; 56/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 56/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 56/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 56/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   2.0s\n",
      "[CV 1/3; 57/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 57/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.389) total time=   2.5s\n",
      "[CV 2/3; 57/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 57/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 57/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 57/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   2.0s\n",
      "[CV 1/3; 58/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 58/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.686, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 58/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 58/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.743, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 58/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 58/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.833, test=0.529) total time=   1.1s\n",
      "[CV 1/3; 59/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 59/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.389) total time=   2.3s\n",
      "[CV 2/3; 59/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 59/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 59/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 59/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.529) total time=   1.9s\n",
      "[CV 1/3; 60/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 60/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.389) total time=   1.8s\n",
      "[CV 2/3; 60/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 60/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 60/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 60/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   1.4s\n",
      "[CV 1/3; 61/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 61/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.686, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 61/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 61/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.743, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 61/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 61/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.833, test=0.529) total time=   1.1s\n",
      "[CV 1/3; 62/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 62/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.714, test=0.389) total time=   2.0s\n",
      "[CV 2/3; 62/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 62/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.800, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 62/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 62/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.833, test=0.588) total time=   1.5s\n",
      "[CV 1/3; 63/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 63/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.389) total time=   2.2s\n",
      "[CV 2/3; 63/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 63/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.6s\n",
      "[CV 3/3; 63/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 63/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   1.8s\n",
      "[CV 1/3; 64/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 64/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.389) total time=   2.2s\n",
      "[CV 2/3; 64/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 64/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.7s\n",
      "[CV 3/3; 64/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 64/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.529) total time=   1.9s\n",
      "[CV 1/3; 65/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 65/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.389) total time=   2.2s\n",
      "[CV 2/3; 65/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 65/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.500) total time=   1.6s\n",
      "[CV 3/3; 65/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 65/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.588) total time=   1.8s\n",
      "[CV 1/3; 66/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 66/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 66/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 66/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.714, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 66/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 66/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.917, test=0.471) total time=   1.3s\n",
      "[CV 1/3; 67/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 67/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.800, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 67/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 67/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.714, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 67/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 67/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.889, test=0.529) total time=   1.2s\n",
      "[CV 1/3; 68/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 68/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.857, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 68/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 68/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.714, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 68/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 68/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.889, test=0.529) total time=   1.2s\n",
      "[CV 1/3; 69/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 69/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.857, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 69/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 69/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.714, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 69/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 69/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.889, test=0.529) total time=   1.3s\n",
      "[CV 1/3; 70/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 70/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 70/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 70/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.714, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 70/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 70/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.917, test=0.471) total time=   1.2s\n",
      "[CV 1/3; 71/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 71/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 71/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 71/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.714, test=0.444) total time=   1.0s\n",
      "[CV 3/3; 71/80] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 71/80] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.722, test=0.471) total time=   1.3s\n",
      "[CV 1/3; 72/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 72/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 72/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 72/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.743, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 72/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 72/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.833, test=0.529) total time=   1.3s\n",
      "[CV 1/3; 73/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 73/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.389) total time=   1.9s\n",
      "[CV 2/3; 73/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 73/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.500) total time=   1.1s\n",
      "[CV 3/3; 73/80] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 73/80] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=1.000, test=0.529) total time=   1.4s\n",
      "[CV 1/3; 74/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 74/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 74/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 74/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.714, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 74/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 74/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.750, test=0.529) total time=   1.3s\n",
      "[CV 1/3; 75/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 75/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 75/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 75/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.714, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 75/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 75/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.750, test=0.529) total time=   1.2s\n",
      "[CV 1/3; 76/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 76/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.686, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 76/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 76/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.714, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 76/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 76/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.750, test=0.529) total time=   1.1s\n",
      "[CV 1/3; 77/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 77/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.686, test=0.389) total time=   1.7s\n",
      "[CV 2/3; 77/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 77/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.714, test=0.500) total time=   1.0s\n",
      "[CV 3/3; 77/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 77/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.917, test=0.471) total time=   1.3s\n",
      "[CV 1/3; 78/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 78/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.686, test=0.389) total time=   1.6s\n",
      "[CV 2/3; 78/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 78/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.714, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 78/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 78/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.917, test=0.471) total time=   1.2s\n",
      "[CV 1/3; 79/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 79/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.686, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 79/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 79/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.714, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 79/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 79/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.806, test=0.529) total time=   1.1s\n",
      "[CV 1/3; 80/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 80/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.686, test=0.389) total time=   1.5s\n",
      "[CV 2/3; 80/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 80/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.714, test=0.500) total time=   0.9s\n",
      "[CV 3/3; 80/80] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 80/80] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.917, test=0.471) total time=   1.1s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 27\n",
      "n_resources: 162\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV 1/3; 1/27] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 1/27] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.645, test=0.593) total time=   3.5s\n",
      "[CV 2/3; 1/27] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 1/27] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.664, test=0.556) total time=   5.8s\n",
      "[CV 3/3; 1/27] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 1/27] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.759, test=0.415) total time=   4.3s\n",
      "[CV 1/3; 2/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 2/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.593) total time=   4.2s\n",
      "[CV 2/3; 2/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 2/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.556) total time=   4.1s\n",
      "[CV 3/3; 2/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 2/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.642) total time=   4.5s\n",
      "[CV 1/3; 3/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 3/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.593) total time=   3.9s\n",
      "[CV 2/3; 3/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 3/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.556) total time=   4.1s\n",
      "[CV 3/3; 3/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 3/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.604) total time=   4.3s\n",
      "[CV 1/3; 4/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 4/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.593) total time=   3.3s\n",
      "[CV 2/3; 4/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 4/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.556) total time=   3.3s\n",
      "[CV 3/3; 4/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 4/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.566) total time=   3.5s\n",
      "[CV 1/3; 5/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 5/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.593) total time=   4.3s\n",
      "[CV 2/3; 5/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 5/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.556) total time=   4.2s\n",
      "[CV 3/3; 5/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 5/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.623) total time=   4.7s\n",
      "[CV 1/3; 6/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 6/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.593) total time=   3.3s\n",
      "[CV 2/3; 6/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 6/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.574) total time=   3.3s\n",
      "[CV 3/3; 6/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 6/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.547) total time=   3.5s\n",
      "[CV 1/3; 7/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 7/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.611) total time=   4.2s\n",
      "[CV 2/3; 7/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 7/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.574) total time=   4.1s\n",
      "[CV 3/3; 7/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 7/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.604) total time=   4.6s\n",
      "[CV 1/3; 8/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 8/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.626, test=0.593) total time=  10.4s\n",
      "[CV 2/3; 8/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 8/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.645, test=0.556) total time=  10.5s\n",
      "[CV 3/3; 8/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 8/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.741, test=0.604) total time=  13.4s\n",
      "[CV 1/3; 9/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 9/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.593) total time=   4.0s\n",
      "[CV 2/3; 9/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 9/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.574) total time=   4.1s\n",
      "[CV 3/3; 9/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 9/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.585) total time=   4.3s\n",
      "[CV 1/3; 10/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 10/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.556) total time=   4.3s\n",
      "[CV 2/3; 10/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 10/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.556) total time=   5.9s\n",
      "[CV 3/3; 10/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 10/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.585) total time=   3.5s\n",
      "[CV 1/3; 11/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 11/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.617, test=0.593) total time=   3.3s\n",
      "[CV 2/3; 11/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 11/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.897, test=0.685) total time=   3.3s\n",
      "[CV 3/3; 11/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 11/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.685, test=0.585) total time=   3.5s\n",
      "[CV 1/3; 12/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 12/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.626, test=0.593) total time=   4.5s\n",
      "[CV 2/3; 12/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 12/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.617, test=0.556) total time=   4.4s\n",
      "[CV 3/3; 12/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 12/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=0.1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.574, test=0.566) total time=   4.7s\n",
      "[CV 1/3; 13/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 13/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.611) total time=   4.0s\n",
      "[CV 2/3; 13/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 13/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.574) total time=   4.1s\n",
      "[CV 3/3; 13/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 13/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.604) total time=   4.2s\n",
      "[CV 1/3; 14/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 14/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.648) total time=   3.4s\n",
      "[CV 2/3; 14/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 14/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.519) total time=   3.3s\n",
      "[CV 3/3; 14/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 14/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.623) total time=   3.4s\n",
      "[CV 1/3; 15/27] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 1/3; 15/27] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.664, test=0.593) total time=   4.2s\n",
      "[CV 2/3; 15/27] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 2/3; 15/27] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.729, test=0.556) total time=   4.0s\n",
      "[CV 3/3; 15/27] START Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n",
      "[CV 3/3; 15/27] END Classifier=SVC(), Classifier__C=10, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.787, test=0.547) total time=   4.4s\n",
      "[CV 1/3; 16/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 16/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.626, test=0.593) total time=   7.1s\n",
      "[CV 2/3; 16/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 16/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.617, test=0.556) total time=   7.0s\n",
      "[CV 3/3; 16/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 16/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.648, test=0.604) total time=   7.8s\n",
      "[CV 1/3; 17/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 17/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.556) total time=   3.3s\n",
      "[CV 2/3; 17/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 17/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.574) total time=   3.3s\n",
      "[CV 3/3; 17/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 17/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=80, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.585) total time=   3.6s\n",
      "[CV 1/3; 18/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 18/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.626, test=0.593) total time=   5.2s\n",
      "[CV 2/3; 18/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 18/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.617, test=0.556) total time=   5.2s\n",
      "[CV 3/3; 18/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 18/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.630, test=0.585) total time=   5.8s\n",
      "[CV 1/3; 19/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 19/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.574) total time=   3.9s\n",
      "[CV 2/3; 19/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 19/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.574) total time=   3.9s\n",
      "[CV 3/3; 19/27] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 19/27] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.623) total time=   7.6s\n",
      "[CV 1/3; 20/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 20/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.626, test=0.593) total time=   5.2s\n",
      "[CV 2/3; 20/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 20/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.617, test=0.556) total time=   3.7s\n",
      "[CV 3/3; 20/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 20/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=1, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.611, test=0.585) total time=   3.9s\n",
      "[CV 1/3; 21/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 21/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.626, test=0.593) total time=   3.9s\n",
      "[CV 2/3; 21/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 21/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.972, test=0.611) total time=   3.8s\n",
      "[CV 3/3; 21/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 21/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.741, test=0.585) total time=   4.0s\n",
      "[CV 1/3; 22/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 22/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.626, test=0.593) total time=   3.7s\n",
      "[CV 2/3; 22/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 22/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.963, test=0.648) total time=   3.6s\n",
      "[CV 3/3; 22/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 22/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.722, test=0.585) total time=   3.8s\n",
      "[CV 1/3; 23/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 23/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.626, test=0.593) total time=   3.3s\n",
      "[CV 2/3; 23/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 23/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.607, test=0.556) total time=   3.5s\n",
      "[CV 3/3; 23/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 23/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.593, test=0.585) total time=   3.6s\n",
      "[CV 1/3; 24/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 24/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.626, test=0.593) total time=   3.8s\n",
      "[CV 2/3; 24/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 24/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.916, test=0.611) total time=   3.7s\n",
      "[CV 3/3; 24/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 24/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.694, test=0.585) total time=   4.1s\n",
      "[CV 1/3; 25/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 25/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.626, test=0.593) total time=   3.6s\n",
      "[CV 2/3; 25/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 25/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.916, test=0.611) total time=   3.5s\n",
      "[CV 3/3; 25/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 25/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.694, test=0.585) total time=   3.7s\n",
      "[CV 1/3; 26/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 26/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.626, test=0.593) total time=   3.7s\n",
      "[CV 2/3; 26/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 26/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.607, test=0.556) total time=   3.8s\n",
      "[CV 3/3; 26/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 26/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.639, test=0.585) total time=   4.3s\n",
      "[CV 1/3; 27/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 27/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.626, test=0.593) total time=   3.9s\n",
      "[CV 2/3; 27/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 27/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.607, test=0.556) total time=   3.9s\n",
      "[CV 3/3; 27/27] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 27/27] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.639, test=0.585) total time=   4.3s\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 9\n",
      "n_resources: 486\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV 1/3; 1/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 1/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.594, test=0.568) total time=  12.6s\n",
      "[CV 2/3; 1/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 1/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.820, test=0.562) total time=  11.0s\n",
      "[CV 3/3; 1/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 1/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.590, test=0.609) total time=  17.6s\n",
      "[CV 1/3; 2/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 2/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.594, test=0.568) total time=  11.9s\n",
      "[CV 2/3; 2/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 2/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.780, test=0.562) total time=  10.2s\n",
      "[CV 3/3; 2/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 2/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 3);, score=(train=0.586, test=0.615) total time=  11.6s\n",
      "[CV 1/3; 3/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 3/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.594, test=0.568) total time=  11.0s\n",
      "[CV 2/3; 3/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 3/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.669, test=0.537) total time=  10.1s\n",
      "[CV 3/3; 3/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 3/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=1, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.583, test=0.615) total time=  11.0s\n",
      "[CV 1/3; 4/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 4/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.617) total time=  13.1s\n",
      "[CV 2/3; 4/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 4/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.512) total time=  10.0s\n",
      "[CV 3/3; 4/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 4/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.627) total time=  10.8s\n",
      "[CV 1/3; 5/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 5/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.630) total time=  11.7s\n",
      "[CV 2/3; 5/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 5/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.512) total time=  12.5s\n",
      "[CV 3/3; 5/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 5/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=500, Feature selection__percentile=40, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.634) total time=  11.1s\n",
      "[CV 1/3; 6/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 1/3; 6/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.617) total time=   9.0s\n",
      "[CV 2/3; 6/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 6/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.519) total time=   8.5s\n",
      "[CV 3/3; 6/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 6/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=2, Classifier__n_estimators=100, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=1.000, test=0.646) total time=   8.9s\n",
      "[CV 1/3; 7/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 7/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.593) total time=  10.9s\n",
      "[CV 2/3; 7/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 7/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.512) total time=  10.4s\n",
      "[CV 3/3; 7/9] START Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 7/9] END Classifier=RandomForestClassifier(random_state=42), Classifier__min_samples_split=3, Classifier__n_estimators=500, Feature selection__percentile=60, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=1.000, test=0.627) total time=  11.0s\n",
      "[CV 1/3; 8/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 8/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.594, test=0.568) total time=  11.5s\n",
      "[CV 2/3; 8/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 8/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.669, test=0.537) total time=  10.6s\n",
      "[CV 3/3; 8/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 8/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 2);, score=(train=0.590, test=0.609) total time=  11.3s\n",
      "[CV 1/3; 9/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 9/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.588, test=0.562) total time=  15.2s\n",
      "[CV 2/3; 9/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 9/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.666, test=0.543) total time=   9.1s\n",
      "[CV 3/3; 9/9] START Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 9/9] END Classifier=LogisticRegression(l1_ratio=0.5, penalty='elasticnet', random_state=42,\n",
      "                   solver='saga'), Classifier__C=10, Feature selection__percentile=20, Preprocessing__CountVectorizer__min_df=2, Preprocessing__CountVectorizer__ngram_range=(1, 1);, score=(train=0.577, test=0.596) total time=   9.3s\n"
     ]
    }
   ],
   "source": [
    "#### CÃ³digo aquÃ­ ####\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "params = [\n",
    "    {\n",
    "    'Preprocessing__CountVectorizer__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'Preprocessing__CountVectorizer__min_df': [1, 2],\n",
    "    'Feature selection__percentile': [20, 40, 60, 80],\n",
    "    'Classifier': [SVC()],\n",
    "    'Classifier__C': [0.1, 1, 10]\n",
    "    },\n",
    "\n",
    "    {\n",
    "    'Preprocessing__CountVectorizer__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'Preprocessing__CountVectorizer__min_df': [1, 2],\n",
    "    'Feature selection__percentile': [20, 40, 60, 80],\n",
    "    'Classifier': [RandomForestClassifier(random_state=42)],\n",
    "    'Classifier__n_estimators': [100, 500],\n",
    "    'Classifier__min_samples_split': [2, 3]\n",
    "    },\n",
    "\n",
    "    {\n",
    "    'Preprocessing__CountVectorizer__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'Preprocessing__CountVectorizer__min_df': [1, 2],\n",
    "    'Feature selection__percentile': [20, 40, 60, 80],\n",
    "    'Classifier': [LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, random_state=42)],\n",
    "    'Classifier__C': [0.1, 1, 10]\n",
    "    }\n",
    "]\n",
    "\n",
    "search = HalvingGridSearchCV(\n",
    "    super_identifier_pipe,\n",
    "    params,\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    verbose=10).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9cMIMZYR1OB",
    "outputId": "bd22ff23-aa36-4f5f-c939-f50a2ddc9b37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Classifier': RandomForestClassifier(random_state=42),\n",
       " 'Classifier__min_samples_split': 2,\n",
       " 'Classifier__n_estimators': 100,\n",
       " 'Feature selection__percentile': 60,\n",
       " 'Preprocessing__CountVectorizer__min_df': 2,\n",
       " 'Preprocessing__CountVectorizer__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "807d969e26fc4b049be6482e527b12a4",
    "deepnote_cell_height": 70.796875,
    "deepnote_cell_type": "markdown",
    "id": "8Fo7uBK7jO0R",
    "tags": []
   },
   "source": [
    "<!-- Reporte la mejor combinaciÃ³n encontrada y justifique por quÃ© cree que es la mejor segÃºn el clasificador usado, la cantidad de columnas seleccionadas y los parÃ¡metros de CountVectorizer seleccionados por GridSearch. -->\n",
    "\n",
    "```\n",
    "La bÃºsqueda entregÃ³ los siguientes parÃ¡metros como los mejores:\n",
    "- Clasificador: Random Forest\n",
    "- min_sample_split: 2\n",
    "- n_estimators: 100\n",
    "- percentile: 60\n",
    "- min_df: 2\n",
    "- ngram_range: (1,1)\n",
    "\n",
    "\n",
    "El clasificador SVM necesita encontrar vectores de soporte para realizar sus predicciones, pero ante la alta dimensionalidad del problema no logra ajustar los vectores de soporte mejor que RandomForest.\n",
    "Adicionalmente el resultado de Support Vector Machine se ve influenciado por el factor de regularizaciÃ³n C, el cual no fue ajustado con mucho detalle en la ejecuciÃ³n de este cÃ³digo.\n",
    "\n",
    "Por parte de Random Forest, el caracter aleatorio pudo facilitar la obtenciÃ³n de mejores resultados, mitigando un poco el efecto de no ajustar muchos parÃ¡metros de forma exhaustiva.\n",
    "\n",
    "En cuanto al clasificador de regresiÃ³n logÃ­stica\n",
    "\n",
    "Por parte de la regression logÃ­stica el tipo de penalizaciÃ³n, solver y demÃ¡s parÃ¡metros a utilizar tambiÃ©n afectan al problema.\n",
    "Por ende puede que al no seleccionarlos de forma cuidadosa, el rendimiento haya sido inferior a Random Forest.\n",
    "\n",
    "El percentil de caracterÃ­sticas utilizado llegÃ³ a 60, posiblemente porque 80 serÃ­an demasiadas caracterÃ­sticas para ajustarse y 40 o 20 muy pocas para equipaprarse al nivel de predicciÃ³n obtenido con 60.\n",
    "\n",
    "El rango de n-gramas se mantuvo en 1, siendo una razÃ³n posible que al aumentar los n-gramas se generan demasiadas caracterÃ­sticas y los datos no son suficientes para capturar su comportamiento (undefitting).\n",
    "\n",
    "En cuanto al min_df se determina que entre 1 y 2, el umbral necesario para que sea considerada como caractterÃ­stica es 1.\n",
    "Posiblemente por la naturaleza del problema se detecten palabras individuales asociadas a comportamientos buenos, neutrales o malignos, sin necesitar palabras compuestas.\n",
    "TambiÃ©n debido al uso del percetil de caracterÃ­sticas se redujo la cantidad de palabras individuales que no aportaban, y como son menos que las de un bigrama no se necesitan tantos datos para generar una buena predicciÃ³n.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e4fd03fac45b4d288f6c67f8bbbf8c18",
    "deepnote_cell_height": 600.15625,
    "deepnote_cell_type": "markdown",
    "id": "OmQUw2aZ_6z2"
   },
   "source": [
    "## 1.4 PredicciÃ³n del datos sin etiquetado  [0.5 puntos]\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://pbs.twimg.com/media/DolotxUUYAAbg7f.jpg\" width=\"350\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7f4aa54f58fd436f99c206aab5be6850",
    "deepnote_cell_height": 111.171875,
    "deepnote_cell_type": "markdown",
    "id": "Cj0ERBgTBFWN"
   },
   "source": [
    "LLego el momento de predecir \n",
    "`Vergil`, `Gorilla Girl` y `Batcow`\n",
    "\n",
    "\n",
    "**Nota:** Recuerde que pueden existir campos vacios en `history_text`, por lo que se les recomienda borrar los nan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6c3937a8832f4c48bbad30dc1b27d42d",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "id": "ZKyX1Kp5jO0R",
    "tags": []
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cell_id": "56ba92e787044d4c9064a0bd5341842d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "id": "jn5oNaNZjO0R",
    "outputId": "eaaae3bc-6f7c-44aa-c478-8e66e1cfe18f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-04eef509-ef65-4cfe-b8d4-08a3f971af7f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>real_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>overall_score</th>\n",
       "      <th>history_text</th>\n",
       "      <th>powers_text</th>\n",
       "      <th>intelligence_score</th>\n",
       "      <th>strength_score</th>\n",
       "      <th>speed_score</th>\n",
       "      <th>...</th>\n",
       "      <th>has_flight</th>\n",
       "      <th>has_accelerated_healing</th>\n",
       "      <th>has_weapons_master</th>\n",
       "      <th>has_intelligence</th>\n",
       "      <th>has_reflexes</th>\n",
       "      <th>has_super_speed</th>\n",
       "      <th>has_durability</th>\n",
       "      <th>has_stamina</th>\n",
       "      <th>has_agility</th>\n",
       "      <th>has_super_strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>122</td>\n",
       "      <td>Batcow</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>Bat-Cow was originally a cow that was found by...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>529</td>\n",
       "      <td>Gorilla Girl</td>\n",
       "      <td>Fahnbullah Eddy</td>\n",
       "      <td>Fahnbullah Eddy</td>\n",
       "      <td>7</td>\n",
       "      <td>A carnival performer with the ability to turn ...</td>\n",
       "      <td>Gorilla Girl can transform into a talking gori...</td>\n",
       "      <td>90</td>\n",
       "      <td>35</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1368</td>\n",
       "      <td>Vergil</td>\n",
       "      <td>Vergil Sparda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>Vergil, later also known as Nelo Angelo, is on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>75</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>122</td>\n",
       "      <td>Batcow</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>Bat-Cow was originally a cow that was found by...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 82 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04eef509-ef65-4cfe-b8d4-08a3f971af7f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-04eef509-ef65-4cfe-b8d4-08a3f971af7f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-04eef509-ef65-4cfe-b8d4-08a3f971af7f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    Unnamed: 0          name        real_name        full_name overall_score  \\\n",
       "16         122        Batcow             None             None             3   \n",
       "40         529  Gorilla Girl  Fahnbullah Eddy  Fahnbullah Eddy             7   \n",
       "78        1368        Vergil    Vergil Sparda              NaN            16   \n",
       "83         122        Batcow             None             None             3   \n",
       "\n",
       "                                         history_text  \\\n",
       "16  Bat-Cow was originally a cow that was found by...   \n",
       "40  A carnival performer with the ability to turn ...   \n",
       "78  Vergil, later also known as Nelo Angelo, is on...   \n",
       "83  Bat-Cow was originally a cow that was found by...   \n",
       "\n",
       "                                          powers_text  intelligence_score  \\\n",
       "16                                                NaN                  70   \n",
       "40  Gorilla Girl can transform into a talking gori...                  90   \n",
       "78                                                NaN                  90   \n",
       "83                                                NaN                  70   \n",
       "\n",
       "    strength_score  speed_score  ...  has_flight  has_accelerated_healing  \\\n",
       "16              10           25  ...         0.0                      0.0   \n",
       "40              35           60  ...         0.0                      0.0   \n",
       "78              75           95  ...         0.0                      1.0   \n",
       "83              10           25  ...         0.0                      0.0   \n",
       "\n",
       "    has_weapons_master has_intelligence has_reflexes has_super_speed  \\\n",
       "16                 0.0              0.0          0.0             0.0   \n",
       "40                 0.0              0.0          0.0             0.0   \n",
       "78                 1.0              1.0          1.0             1.0   \n",
       "83                 0.0              0.0          0.0             0.0   \n",
       "\n",
       "   has_durability has_stamina has_agility  has_super_strength  \n",
       "16            0.0         0.0         0.0                 0.0  \n",
       "40            0.0         0.0         1.0                 1.0  \n",
       "78            1.0         1.0         1.0                 1.0  \n",
       "83            0.0         0.0         0.0                 0.0  \n",
       "\n",
       "[4 rows x 82 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CÃ³digo aquÃ­ ####\n",
    "cleaned_comics_no_label = df_comics_no_label.dropna(subset=[\"history_text\"])\n",
    "to_predict = cleaned_comics_no_label.loc[cleaned_comics_no_label[\"name\"].isin([\"Vergil\", \"Gorilla Girl\", \"Batcow\"])]\n",
    "to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p59VL3wIlrVC",
    "outputId": "5022495d-5a11-4ea8-ffbc-d673ef08367b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Good', 'Good', 'Good', 'Good'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.predict(to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0ee55c847633405fb2e7cfaade1fc799",
    "deepnote_cell_height": 269,
    "deepnote_cell_type": "markdown",
    "id": "Rg4ZMq8ezAH6"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/images/fb5bf7cc5a4acb91b4177672886a88ba/tenor.gif?itemid=5591338\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "id": "aG5mkUDUjO0R",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LCOUC4jss148",
    "GtG74Cphq56p"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "b7ffc7ddb4f14fcd976082c27e48a913",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contenidos",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.867px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
